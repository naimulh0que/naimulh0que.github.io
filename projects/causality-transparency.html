<!DOCTYPE html>
<meta charset="utf-8">
<head>
	<title>Naimul Hoque</title>
	<meta charset="UTF-8">

	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Josefin+Sans:400,400i,600,600i,700" rel="stylesheet">

	<!-- Stylesheets -->
	<link rel="stylesheet" href="../css/bootstrap.min.css"/>
	<link rel="stylesheet" href="../css/font-awesome.min.css"/>
	<link rel="stylesheet" href="../css/magnific-popup.css"/>
	<link rel="stylesheet" href="../css/custom.css"/>
</head>
<style>
.mainBars rect{
  shape-rendering: auto;
  fill-opacity: 0;
  stroke-width: 0.5px;
  stroke: rgb(0, 0, 0);
  stroke-opacity: 0;
}
.subBars{
	shape-rendering:crispEdges;
}
.edges{
	stroke:none;
	fill-opacity:0.5;
}
</style>
<body>
	<nav class="navbar navbar-expand-sm navbar-dark" style="background-color: #1687a7;">
	  <ul class="navbar-nav">
	    <li class="nav-item active">
	      <a id="home" class="nav-link" href="../index.html">Home</a>
	    </li>
	    <li class="nav-item">
	      <a id="projects" class="nav-link" href="../index.html">Projects</a>
	    </li>
	    <li class="nav-item">
	      <a id="publications" class="nav-link" href="../index.html">Publications</a>
	    </li>
	    <li class="nav-item">
	      <a href="../docs/Naimul_Resume.pdf" class="nav-link">CV</a>
	    </li>
	  </ul>
	</nav>
	<div class="container-fluid">
		<div class="row" style="margin-top: 50px;margin-left: 20px;">
			<div class="col-lg-12">
				<!-- <a href="../index.html"> Home</a> -->
		    </div>
		</div>
		<div class="row" style="margin-top: 50px;margin-left: 20px;">
			<div class="col-lg-8">
				<h3>Outcome-Explorer: A Causality Guided Interactive Visual Interface for Interpretable Algorithmic Decision Making</h3> (Under review)
				<p>Naimul Hoque, Klaus Mueller</p>
				<p>The widespread adoption of algorithmic decision-making systems has brought about the necessity to interpret the reasoning behind these decisions. The majority of these systems are complex black box models, and auxiliary models are often used to approximate and then explain their behavior. However, recent research suggests that such explanations are not overly accessible to non-expert users and can lead to incorrect interpretation of the underlying model. In this paper, we show that a predictive and interactive model based on causality is inherently interpretable, does not require any auxiliary model, and allows both expert and non-expert users to understand the model comprehensively. To demonstrate our method we developed Outcome Explorer, a causality guided interactive interface, and evaluated it by conducting think-aloud sessions with three expert users and a user study with 18 non-expert users. All three expert users found our tool to be comprehensive in supporting their explanation needs while the non-expert users were able to understand the inner workings of the model easily. 
				</p>
			</div>
			
		</div>
		<div class="row" style="margin-top: 50px;margin-left: 20px;">
		
			<div class="col-lg-8">
				<iframe width="560" height="315" src="https://www.youtube.com/embed/oWnnexfiA1k" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
				<!-- <img src="../img/outcome-explorer.png" width="1000">
				<p> <b>Figure</b>: Model Interpretation Module of Outcome-Explorer. A) Interactive causal DAG showing causal relations between variables. Each node includes two circular knobs (green and orange) to facilitate profile comparisons. The edge thickness and color depict the effect size and type of each edge. B) Sample selection panel. C) A biplot showing the position of green and orange profiles compared to nearest neighbors. D) A line chart to track the model outcome and to go back and forth between feature configuration. E) Realism meter allowing users to determine how common a profile is compared to other samples in the dataset.</p> -->
			</div>

		</div>
	</div>

<script src="../js/jquery-2.1.4.min.js"></script>

</body>
</html>
